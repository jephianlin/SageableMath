{"kernelspec":{"display_name":"SageMath (stable)","language":"sagemath","name":"sagemath"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.15"}}
{"cell_type":"markdown","metadata":{},"source":"# Linear Regression\n&mdash;by Jephian Lin"}
{"cell_type":"markdown","metadata":{},"source":"##### Overview\nGiven a set of $N$ data $(x_i,y_i)$ for $i=1,\\ldots,N$,\n**linear regression** aims to find a line \n\n> $y=ax+b$\n\nthat best describes the data.  \n\nThat is, the goal is  \nto find two values $a$ and $b$ such that  \n\n> $\\sum_i^N (y_i - ax_i - b)^2$\n\nis minimized."}
{"cell_type":"markdown","metadata":{},"source":"##### Algorithm\n1. Create an $N\\times 2$ matrix $A=\\begin{bmatrix}x_1 & 1 \\\\ \\vdots & \\vdots \\\\ x_N & 1 \\end{bmatrix}$ and a vector $v=\\begin{bmatrix}y_1 \\\\ \\vdots \\\\ y_N \\end{bmatrix}$.\n2. Then compute \n\n> $\\begin{bmatrix} a \\\\ b \\end{bmatrix} = (A^\\top A)^{-1} A^\\top v$\n\nNote: If $(A^\\top A)^{-1}$ does not exist, use the Penrose&ndash;Moore pseudo inverse instead."}
{"cell_type":"markdown","metadata":{},"source":"##### Explanation\nThe goal is to solve the equation \n\n> $Ax = v$, where $x = \\begin{bmatrix} a \\\\ b \\end{bmatrix}$\n\nfor $x$.  \n\nThe equation does not always have a solution.  \nIf not solvable, we find a vector $x$ such that \n\n> $|Ax-v|^2 = \\sum_i^N (y_i - ax_i - b)^2$ \n\nis minimized.  \n\nTo do so, let $v_0$ be the orthogonal projection of $v$ onto the column space of $A$.  \nBy the formula of orthogonal projection \n\n> $v_0 = A(A^\\top A)^{-1}A^\\top v$.\n\nNow solve $Ax=v_0$ and get $x=(A^\\top A)^{-1}A^\\top v$.  \nTherefore,  \n\n> $\\begin{bmatrix} a \\\\ b \\end{bmatrix} = (A^\\top A)^{-1}A^\\top v$."}
{"cell_type":"markdown","metadata":{},"source":"##### Implimentation"}
{"cell_type":"code","execution_count":1,"metadata":{"trusted":true},"outputs":[],"source":"def linear_regression(data, draw=False):\n    \"\"\"\n    Input:\n        data: a list of pairs [(x1,y1), ..., (xN,yN)]\n    Output:\n        Output [a,b] so that the line y = ax + b  \n        is the best fitting line for the data.\n        When draw == True, \n        create a graphical illustration p and return [a,b,p].\n    \"\"\"\n    NN = len(data)\n    ### x_list = [x1, x2, ..., xN]\n    x_list = [p[0] for p in data]\n    ### one_list = [1,1, ..., 1]\n    one_list = [1] * NN\n    ### y_list = [y1, y2, ..., yN]\n    y_list = [p[1] for p in data]\n    ### define A and v as described in the algorithm\n    A = matrix([x_list, one_list]).transpose()\n    v = matrix([y_list]).transpose()\n    AT = A.transpose()\n    ATA = AT * A\n    ATAinv = ATA.pseudoinverse()\n    ans = ATAinv * AT * v\n    a, b = ans.transpose()[0]\n    \n    if draw:\n        x_min = min(x_list)\n        x_max = max(x_list)\n        x_range = x_max - x_min\n        x = var('x')\n        pic = (a*x + b).plot(xmin=x_min-0.1*x_range, \n                             xmax=x_max+0.1*x_range)\n        pic += point(data, rgbcolor='red', size=30)\n\n        return [a,b,pic]\n    \n    return [a,b]"}
{"cell_type":"markdown","metadata":{},"source":"##### Examples"}
{"cell_type":"code","execution_count":2,"metadata":{"scrolled":true,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"(0, 1)\n"},{"data":{"image/png":"smc-blob::472f4e06-4545-4273-a577-62e10f7936e3","text/plain":"Graphics object consisting of 2 graphics primitives"},"metadata":{},"output_type":"display_data"}],"source":"### horizontal data\ndata = [(1,1),(2,1),(3,1),(4,1),(5,1)] \n\na,b,p = linear_regression(data,True)\nprint(a,b)\np.show()"}
{"cell_type":"code","execution_count":19,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"(0.500000000000000, 0.500000000000000)\n"},{"data":{"image/png":"smc-blob::588c274f-e69f-4601-815a-a1b89c9e75ff","text/plain":"Graphics object consisting of 2 graphics primitives"},"metadata":{},"output_type":"display_data"}],"source":"### linear data\ndata = [(1,1),(2,1.5),(3,2),(4,2.5),(5,3)] \n\na,b,p = linear_regression(data,True)\nprint(a,b)\np.show()"}
{"cell_type":"code","execution_count":20,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"(1/2, 3/10)\n"},{"data":{"image/png":"smc-blob::057826a5-a54b-41f3-8b05-be05a8de5997","text/plain":"Graphics object consisting of 2 graphics primitives"},"metadata":{},"output_type":"display_data"}],"source":"### non-linear data\ndata = [(1,1),(2,1),(3,2),(4,2),(5,3)] \n\na,b,p = linear_regression(data,True)\nprint(a,b)\np.show()"}
{"cell_type":"code","execution_count":21,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"(0.1910367696500197, 1.0160041016768095)\n"},{"data":{"image/png":"smc-blob::9d169566-9ee2-412f-8c38-9db3b8dd596a","text/plain":"Graphics object consisting of 2 graphics primitives"},"metadata":{},"output_type":"display_data"}],"source":"### almost-linear data\nimport numpy as np\nx = np.linspace(1,5,50)\ny = x*0.2 + 1 + 0.1*np.random.randn(50)\ndata = list(zip(x,y))\n\na,b,p = linear_regression(data,True)\nprint(a,b)\np.show()"}
{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":""}