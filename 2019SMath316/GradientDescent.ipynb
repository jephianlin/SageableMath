{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by/4.0/88x31.png\" /></a><br />This work by <span xmlns:cc=\"http://creativecommons.org/ns#\" property=\"cc:attributionName\">吳安容</span> is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\">Creative Commons Attribution 4.0 International License</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Overview\n",
    "Let $f:\\mathbb{R}^2\\rightarrow\\mathbb{R}$ be a function. \n",
    "Our main goal is to find a local minimum of a function $f$. \n",
    "\n",
    "We use the method of **gradient descent** to find a local minimum of $f$. \n",
    "\n",
    "Define a function $f$ from $\\mathbb{R}^2$ to $\\mathbb{R}$. \n",
    "\n",
    "For example, when $f(x,y)=x^2+y^2$, the algorithm will return an approximation of the location of the minimum $(0,0)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Algorithm\n",
    "1. Let $f$ be a function from $\\mathbb{R}^2$ to $\\mathbb{R}$ and choose $\\mathbf{x}_0=(x_0,y_0)=(0,0)$ as an initial point.\n",
    "2. Then compute $\\nabla f(x_i,y_i)$, the gradient of $f$ at $\\mathbf{x}_i$, and\n",
    "\n",
    "> $\\mathbf{x}_{i+1}=\\mathbf{x}_i-\\alpha\\cdot\\nabla f(x_i,y_i)$  \n",
    "\n",
    "where $\\alpha$ is a small enough number such that the sequence$ (\\mathbf{x}_n)$ converges to a local minimum.\n",
    "3. Repeat Step 2 for several steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explanation\n",
    "The goal is to find a local minimum of a given function.\n",
    "\n",
    "Given functions are restricted to be two variables and the gradient of function is needed to be calculated by hands.\n",
    "\n",
    "We set $\\alpha = 0.05$ and the step of $x$ denoted by $\\epsilon < 10^{-5}$.\n",
    "\n",
    "The sequence might converge slowly or not converge, so we need to set the maximum iteration times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Implimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gradient_descent(f, gradf, steps=100, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        f: a function of x,y that we want to evaluate the minimum\n",
    "        gradf:gradient of the function f\n",
    "        steps:the steps of iterations\n",
    "        alpha:step size multiplier\n",
    "    Output:\n",
    "        min:the minimum of f\n",
    "    \"\"\"\n",
    "    v1 = vector(np.random.randn(2)); ### next vector\n",
    "    v2 = vector(np.random.randn(2)); ### current vector\n",
    "    for i in range(steps):\n",
    "        v2 = v1;\n",
    "        v1 = v2 - alpha * vector(gradf(v2));\n",
    "        h = v1[0] - v2[0];\n",
    "        if abs(h) <= epsilon :\n",
    "            break;\n",
    "    print(\"Minimum at\",v1,\" and minimum is\",f(v1));    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 1 \n",
    "$f(x,y)=x^2+y^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Minimum at', (0.0, 0.0), ' and minimum is', 0.0)\n"
     ]
    }
   ],
   "source": [
    "epsilon=0.00001;\n",
    "\n",
    "f = lambda v1: v1[0]^2 + v1[1]^2\n",
    "\n",
    "gradf = lambda v2: (2*v2[0], 2*v2[1])\n",
    "\n",
    "gradient_descent(f,gradf, steps=100, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Minimum at', (1.236371488782072, -0.781210485847116), ' and minimum is', 2.1389042814706842)\n"
     ]
    }
   ],
   "source": [
    "### If we use a large alpha,then the result may be different.\n",
    "\n",
    "gradient_descent(f,gradf, steps=100, alpha=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 2 \n",
    "$f(x,y)=x^2-y^2+xy-4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Minimum at', (2.792893696625991e+31, -1.1830887552838372e+32), ' and minimum is', -1.652120563590659e+64)\n"
     ]
    }
   ],
   "source": [
    "epsilon=0.00001;\n",
    "\n",
    "f = lambda v1: v1[0]^2 - v1[1]^2 + v1[0]*v1[1] - 4\n",
    "\n",
    "gradf = lambda v2: (2*v2[0] + v2[1], -2*v2[1]+v2[0])\n",
    "\n",
    "gradient_descent(f,gradf, steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0,0) + (1,0) = (0, 0, 1, 0)\n",
      "u+v = (1, 0)\n",
      "u[0],u[1] = 1 0\n",
      "f(v) = 0\n",
      "f(u) = 1\n",
      "g(v) = 0\n",
      "g(u) = 1\n"
     ]
    }
   ],
   "source": [
    "### example of vectors and functions in SageMath\n",
    "\n",
    "### tuples are nice but their addition is not what we want\n",
    "print \"(0,0) + (1,0) =\", (0,0) + (1,0)\n",
    "\n",
    "### Use vector instead\n",
    "v = vector([0,0])\n",
    "u = vector([1,0])\n",
    "print \"u+v =\", u+v\n",
    "\n",
    "### get vector entries\n",
    "print \"u[0],u[1] =\", u[0], u[1]\n",
    "\n",
    "### a function that takes a vector as input\n",
    "### syntax:\n",
    "### lambda input: output\n",
    "### this is called the lambda method to define a function\n",
    "f = lambda v: v[0]^2 + v[1]^2\n",
    "print \"f(v) =\", f(v)\n",
    "print \"f(u) =\", f(u)\n",
    "\n",
    "### alternatively, you have to do the classical function define\n",
    "def g(v):\n",
    "    return v[0]^2 + v[1]^2\n",
    "### two methods are almost the same except that the lambda method you don't have to give a name to the function.\n",
    "print \"g(v) =\", g(v)\n",
    "print \"g(u) =\", g(u)\n",
    "\n",
    "### Therefore, the gradf can be\n",
    "gradf = lambda v: 2*v[0] + 2*v[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SageMath 8.1",
   "language": "",
   "name": "sagemath"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
